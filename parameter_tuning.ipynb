{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Selection for UF6 Neural Network\n",
    "This notebook will explore the value of feature engineering and selection for the UF6 GADRAS data set.\n",
    "We will explore the predictor correlations, down-select predictors based on correlation, cross-validate to choose model hyperparameters and finally train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all of the libraries \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import base\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copied from the http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "# This defines a function to build a confusion matrix, which we will use later.\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "   # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def forest_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    n_est = [500]\n",
    "#    n_est = [500,900,1000,1100,1200]\n",
    "#    max_feat = [31, 61, 91, 121, 151, 181, 230, 400]\n",
    "    max_feat = [50]\n",
    "#    max_feat = [40,45,50,55,60,65,70,75,80]\n",
    "    min_samp_split = [2]\n",
    "    min_samp_leaf = [1]\n",
    "    names = []\n",
    "    classifiers = []\n",
    "    for i in n_est:\n",
    "        for j in max_feat:\n",
    "            for k in min_samp_split:\n",
    "                for l in min_samp_leaf:\n",
    "                    names.append('n_est='+str(i)+' max_feat=' +str(j) + ' min_samp_split=' +str(k) +' min_samp_leaf=' +str(l))\n",
    "                    classifiers.append(RandomForestClassifier(n_estimators=i, max_features=j, min_samples_split=k,\n",
    "                                                min_samples_leaf=l))\n",
    "    # iterate over classifiers\n",
    "    y = []\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        score = clf.score(X_test, y_test.values.ravel())\n",
    "        y.append(score)\n",
    "        print(name, score)\n",
    "    return(zip(names, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def forest_CVreg(columns, X_train, X_test, y_train, y_test):\n",
    "    n_est = [500]\n",
    "#    n_est = [500,900,1000,1100,1200]\n",
    "#    max_feat = [31, 61, 91, 121, 151, 181, 230, 400]\n",
    "    max_feat = [50]\n",
    "#    max_feat = [40,45,50,55,60,65,70,75,80]\n",
    "    min_samp_split = [2]\n",
    "    min_samp_leaf = [1]\n",
    "    names = []\n",
    "    classifiers = []\n",
    "    for i in n_est:\n",
    "        for j in max_feat:\n",
    "            for k in min_samp_split:\n",
    "                for l in min_samp_leaf:\n",
    "                    names.append('n_est='+str(i)+' max_feat=' +str(j) + ' min_samp_split=' +str(k) +' min_samp_leaf=' +str(l))\n",
    "                    classifiers.append(RandomForestRegressor(n_estimators=i, max_features=j, min_samples_split=k,\n",
    "                                                min_samples_leaf=l))\n",
    "    # iterate over classifiers\n",
    "    y = []\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        score = clf.score(X_test, y_test.values.ravel())\n",
    "        y.append(score)\n",
    "        print(name, score)\n",
    "    return(zip(names, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this class will allow us to select different features, which are columns in the data set\n",
    "# it is necessary to create a Class of this type for use in Pipelines\n",
    "\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # return a new array with just the columns you specify\n",
    "        newarray = X.filter(self.col_names, axis=1)\n",
    "        return newarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mooreet_la\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (130) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "#data_df = pd.read_csv('/Users/mooreet_la/projects/SDRD/competition/data/TNG_set.csv')\n",
    "data_df = pd.read_csv('/Users/mooreet_la/projects/SDRD/competition/data/TNG_set128_v2.csv')\n",
    "labels = data_df['SourceID'].copy()\n",
    "locations = data_df['location'].copy()\n",
    "channel_data = data_df.iloc[:, 0:128].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23', 'Channel24', 'Channel25', 'Channel26', 'Channel27', 'Channel28', 'Channel29', 'Channel30', 'Channel31', 'Channel32', 'Channel33', 'Channel34', 'Channel35', 'Channel36', 'Channel37', 'Channel38', 'Channel39', 'Channel40', 'Channel41', 'Channel42', 'Channel43', 'Channel44', 'Channel45', 'Channel46', 'Channel47', 'Channel48', 'Channel49', 'Channel50', 'Channel51', 'Channel52', 'Channel53', 'Channel54', 'Channel55', 'Channel56', 'Channel57', 'Channel58', 'Channel59', 'Channel60', 'Channel61', 'Channel62', 'Channel63', 'Channel64', 'Channel65', 'Channel66', 'Channel67', 'Channel68', 'Channel69', 'Channel70', 'Channel71', 'Channel72', 'Channel73', 'Channel74', 'Channel75', 'Channel76', 'Channel77', 'Channel78', 'Channel79', 'Channel80', 'Channel81', 'Channel82', 'Channel83', 'Channel84', 'Channel85', 'Channel86', 'Channel87', 'Channel88', 'Channel89', 'Channel90', 'Channel91', 'Channel92', 'Channel93', 'Channel94', 'Channel95', 'Channel96', 'Channel97', 'Channel98', 'Channel99', 'Channel100', 'Channel101', 'Channel102', 'Channel103', 'Channel104', 'Channel105', 'Channel106', 'Channel107', 'Channel108', 'Channel109', 'Channel110', 'Channel111', 'Channel112', 'Channel113', 'Channel114', 'Channel115', 'Channel116', 'Channel117', 'Channel118', 'Channel119', 'Channel120', 'Channel121', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128']\n",
      "128\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# define the set of predictors you want to use to train the model\n",
    "\n",
    "features = list(channel_data) # takes the engineered features\n",
    "#features.remove('city')\n",
    "cst = ColumnSelectTransformer(features)\n",
    "\n",
    "print(features)\n",
    "print(len(features))\n",
    "print(type(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(locations.head())\n",
    "# print(locations.shape)\n",
    "# print(labels.head())\n",
    "# print(channel_data.head())\n",
    "# print(channel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df1 = pd.DataFrame(np.random.randn(6,4),\n",
    "#                index=list('abcdef'),                 columns=list('ABCD'))\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# f = df1.loc['d':, 'A':'C']\n",
    "# f\n",
    "# #g = df1.iloc[3:,0:3]\n",
    "# #g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: SourceID, dtype: int64\n",
      "   SourceID\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n"
     ]
    }
   ],
   "source": [
    "junk = labels.copy()\n",
    "hasattr(labels,'filter')\n",
    "print(type(labels))\n",
    "print(type(junk))\n",
    "junk = pd.DataFrame(data=junk, index=None)\n",
    "print(type(junk))\n",
    "print(labels.head())\n",
    "print(junk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labelsDFtest = labels.copy()\n",
    "# print(labelsDFtest.shape)\n",
    "# print(labelsDFtest.head())\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 0, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 1, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 2, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 3, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 4, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 5, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 6, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# print(labelsDFtest.shape)\n",
    "# print(labelsDFtest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(labelsDFtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Bkg\n",
      "1    Bkg\n",
      "2    Bkg\n",
      "3    Bkg\n",
      "4    Bkg\n",
      "Name: SourceID, dtype: object\n",
      "(39005,)\n",
      "<class 'numpy.ndarray'>\n",
      "(39005, 1)\n"
     ]
    }
   ],
   "source": [
    "#featkeys = channel_data['SourceIDs']\n",
    "#featkeys = np.reshape(featkeys, (29000,1))\n",
    "\n",
    "#featdata = featdata.drop('SourceIDs', axis=1)\n",
    "# print(featdata.shape)\n",
    "# print(featkeys.shape)\n",
    "\n",
    "labels = labels.replace(to_replace= 0, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 1, value='HEU', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 2, value='WGPu', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 3, value='I131', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 4, value='Co60', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 5, value='Tc99m', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 6, value='Tc+HEU', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "\n",
    "# for i in range(0,labels.shape[0]):\n",
    "#     if labels[i] == 0:\n",
    "#         labels[i] = 'Bkg'\n",
    "#     elif labels[i] == 1:\n",
    "#         labels[i] = 'HEU'\n",
    "#     elif labels[i] == 2:\n",
    "#         labels[i] = 'WGPu'\n",
    "#     elif labels[i] == 3:\n",
    "#         labels[i] = 'I131'\n",
    "#     elif labels[i] == 4:\n",
    "#         labels[i] = 'Co60'\n",
    "#     elif labels[i] == 5:\n",
    "#         labels[i] = 'Tc99m'\n",
    "#     else:\n",
    "#         labels[i] = 'Tc+HEU'\n",
    "        \n",
    "print(labels.head())\n",
    "print(labels.shape)\n",
    "labels = labels.values.reshape([labels.shape[0],1])\n",
    "print(type(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 1)\n",
      "(39005, 128)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "junk = labels.copy()\n",
    "junk = pd.DataFrame(data=junk, index=None)\n",
    "featkeys = junk.copy()\n",
    "featdata = channel_data.copy()\n",
    "\n",
    "print(featkeys.shape)\n",
    "print(featdata.shape)\n",
    "print(type(labels))\n",
    "print(type(featkeys))\n",
    "print(type(featdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will perform cross validations to select hyperparameters for four different models: \n",
    "full channel, subset channel, full feature, subset feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess dataset, split into training and test part\n",
    "def data_split(data, keys, columns):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, keys, \n",
    "                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(X_train)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(X_test)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transform(dataTRN, dataTEST, columns):\n",
    "    # just transform the already split data    ### This is redundant if you already used the split command!!\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(dataTRN)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(dataTEST)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_fullfeat, X_train, X_test, y_train, y_test = data_split(featdata, featkeys, features)\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat = data_split(featdata, featkeys, subset_features)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(featdata, featkeys, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "# y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26133, 128)\n",
      "(12872, 128)\n",
      "(39005, 128)\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#print(X_train_fullfeat.shape)\n",
    "#print(X_test_fullfeat.shape)\n",
    "#print(y_train_fullfeat.shape)\n",
    "#print(y_test_fullfeat.shape)\n",
    "featdataTRN = X_train.copy()\n",
    "featdataTES = X_test.copy()\n",
    "featkeysTRN = y_train.copy()\n",
    "featkeysTES = y_test.copy()\n",
    "print(featdataTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featdata.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(type(X_train))\n",
    "print(type(featdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12872, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(featkeysTES.shape)\n",
    "print(type(featkeysTES))\n",
    "print(type(featdataTRN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #  only if not splitting ????\n",
    "# col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "# y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES\n",
    "# #col_subfeat, X_train_subfeat, X_test_subfeat = data_transform(featdataTRN, featdataTES, subset_features)\n",
    "# #y_train_subfeat, y_test_subfeat = featkeysTRN, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "(12872, 1)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#featkeysTES = 0\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "(26133, 128)\n",
      "(12872, 128)\n"
     ]
    }
   ],
   "source": [
    "# take a quick look, these should have the same number of rows\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "#print(featkeys1.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featdataTES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the split data with the PCR & PLS in the dataframe\n",
    "\n",
    "#featdata = featdataTRN\n",
    "#featkeys = featkeysTRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1</th>\n",
       "      <th>Channel2</th>\n",
       "      <th>Channel3</th>\n",
       "      <th>Channel4</th>\n",
       "      <th>Channel5</th>\n",
       "      <th>Channel6</th>\n",
       "      <th>Channel7</th>\n",
       "      <th>Channel8</th>\n",
       "      <th>Channel9</th>\n",
       "      <th>Channel10</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel119</th>\n",
       "      <th>Channel120</th>\n",
       "      <th>Channel121</th>\n",
       "      <th>Channel122</th>\n",
       "      <th>Channel123</th>\n",
       "      <th>Channel124</th>\n",
       "      <th>Channel125</th>\n",
       "      <th>Channel126</th>\n",
       "      <th>Channel127</th>\n",
       "      <th>Channel128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>134</td>\n",
       "      <td>141</td>\n",
       "      <td>89</td>\n",
       "      <td>71</td>\n",
       "      <td>66</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>98</td>\n",
       "      <td>171</td>\n",
       "      <td>139</td>\n",
       "      <td>102</td>\n",
       "      <td>107</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>106</td>\n",
       "      <td>165</td>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>115</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>108</td>\n",
       "      <td>179</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>68</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>96</td>\n",
       "      <td>162</td>\n",
       "      <td>170</td>\n",
       "      <td>98</td>\n",
       "      <td>118</td>\n",
       "      <td>93</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel1  Channel2  Channel3  Channel4  Channel5  Channel6  Channel7  \\\n",
       "0         5        28        90       150       134       141        89   \n",
       "1         4        30        98       171       139       102       107   \n",
       "2         7        31       106       165       134       131       115   \n",
       "3         3        34       108       179       142       129        85   \n",
       "4        10        31        96       162       170        98       118   \n",
       "\n",
       "   Channel8  Channel9  Channel10     ...      Channel119  Channel120  \\\n",
       "0        71        66         59     ...               0           0   \n",
       "1        99        65         62     ...               0           1   \n",
       "2        84        70         57     ...               0           0   \n",
       "3        85        68         65     ...               0           0   \n",
       "4        93        63         53     ...               0           1   \n",
       "\n",
       "   Channel121  Channel122  Channel123  Channel124  Channel125  Channel126  \\\n",
       "0           1           1           0           0           0           1   \n",
       "1           0           0           0           1           0           0   \n",
       "2           0           0           1           0           0           1   \n",
       "3           0           0           1           0           0           1   \n",
       "4           0           0           1           1           0           0   \n",
       "\n",
       "   Channel127  Channel128  \n",
       "0           3           2  \n",
       "1           1           0  \n",
       "2           2           0  \n",
       "3           3           1  \n",
       "4           0           1  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39005, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featkeys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will perform cross validations to select hyperparameters for different models: \n",
    "full 128 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best value of alpha\n",
    "\n",
    "def alpha_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    alphas = np.logspace(-5, 3, 9)\n",
    "    names = []\n",
    "    for i in alphas:\n",
    "        names.append('alpha ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in alphas:\n",
    "        classifiers.append(MLPClassifier(solver='adam', activation='tanh', alpha=i, random_state=1))\n",
    "\n",
    "    x = alphas\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "    \n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(alphas, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best value of alpha\n",
    "\n",
    "def alpha_CVreg(columns, X_train, X_test, y_train, y_test):\n",
    "    alphas = np.logspace(-5, 3, 9)\n",
    "    names = []\n",
    "    for i in alphas:\n",
    "        names.append('alpha ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in alphas:\n",
    "        classifiers.append(MLPRegressor(solver='adam', activation='tanh', alpha=i, random_state=1))\n",
    "\n",
    "    x = alphas\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "    \n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(alphas, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best solver\n",
    "\n",
    "def solver_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    solvers = ['lbfgs','sgd','adam']\n",
    "    names = []\n",
    "    for i in solvers:\n",
    "        names.append('solver= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in solvers:\n",
    "        classifiers.append(MLPClassifier(solver=i, activation='tanh', learning_rate='adaptive', alpha=0.1, random_state=1))\n",
    "        \n",
    "    x = [1,2,3]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(solvers, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best solver\n",
    "\n",
    "def solver_CVreg(columns, X_train, X_test, y_train, y_test):\n",
    "    solvers = ['lbfgs','sgd','adam']\n",
    "    names = []\n",
    "    for i in solvers:\n",
    "        names.append('solver= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in solvers:\n",
    "        classifiers.append(MLPRegressor(solver=i, activation='tanh', learning_rate='adaptive', alpha=0.1, random_state=1))\n",
    "        \n",
    "    x = [1,2,3]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(solvers, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def activation_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    activators = ['identity', 'logistic', 'tanh', 'relu']\n",
    "    names = []\n",
    "    for i in activators:\n",
    "        names.append('activator= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in activators:\n",
    "        classifiers.append(MLPClassifier(solver='adam', activation=i, alpha=0.1, random_state=1))\n",
    "\n",
    "    x = [1,2,3,4]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(activators, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def activation_CVreg(columns, X_train, X_test, y_train, y_test):\n",
    "    activators = ['identity', 'logistic', 'tanh', 'relu']\n",
    "    names = []\n",
    "    for i in activators:\n",
    "        names.append('activator= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in activators:\n",
    "        classifiers.append(MLPRegressor(solver='adam', activation=i, alpha=0.1, random_state=1))\n",
    "\n",
    "    x = [1,2,3,4]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(activators, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess dataset, split into training and test part\n",
    "def data_split(data, keys, columns):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, keys, \n",
    "                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(X_train)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(X_test)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std, y_train, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transform(dataTRN, dataTEST, columns):\n",
    "    # just transform the already split data\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(dataTRN)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(dataTEST)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat = data_split(featdata, featkeys, features)\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat = data_split(featdata, featkeys, subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "(12872, 1)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#featkeysTES = 0\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "#y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat = data_transform(featdataTRN, featdataTES, subset_features)\n",
    "#y_train_subfeat, y_test_subfeat = featkeysTRN, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mooreet_la\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:912: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\mooreet_la\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.0000000000000001e-05, 0.5682877563704164), (0.0001, 0.5682877563704164), (0.001, 0.56968614045991295), (0.01, 0.57636730888750776), (0.10000000000000001, 0.61839651957737729), (1.0, 0.71177750155376007), (10.0, 0.65273461777501551), (100.0, 0.41019266625233064), (1000.0, 0.38906152889993784)]\n",
      "[('lbfgs', 0.55445929148539463), ('sgd', 0.70455251709136113), ('adam', 0.61839651957737729)]\n",
      "[('identity', 0.70400870105655688), ('logistic', 0.71022374145431943), ('tanh', 0.61839651957737729), ('relu', 0.63408949658172775)]\n"
     ]
    }
   ],
   "source": [
    "alpha_full_feat = alpha_CV(features, X_train, X_test, y_train, y_test)\n",
    "print(list(alpha_full_feat))\n",
    "\n",
    "solver_full_feat = solver_CV(features, X_train, X_test, y_train, y_test)\n",
    "print(list(solver_full_feat))\n",
    "\n",
    "activation_full_feat = activation_CV(features, X_train, X_test, y_train, y_test)\n",
    "print(list(activation_full_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_est=500 max_feat=50 min_samp_split=2 min_samp_leaf=1 0.703309509012\n"
     ]
    }
   ],
   "source": [
    "forest_full_feat = forest_CV(features, X_train, X_test, y_train, y_test)\n",
    "#print(list(forest_full_feat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finallly we will train four different models on the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just read in data from files and over-write\n",
    "# featkeysTRN = pd.read_csv('/home/jack/projects/dengue/data/sj_train100per_labels.csv')\n",
    "\n",
    "# featdataTRN = pd.read_csv('/home/jack/projects/dengue/data/sj_train100per.csv')\n",
    "# featdataTES = pd.read_csv('/home/jack/projects/dengue/data/sj_testTrue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(type(featdata))\n",
    "print(type(featkeys))\n",
    "print(type(featdataTRN))\n",
    "print(type(featkeysTRN))\n",
    "print(type(featdataTES))\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline which you can use to train and predict\n",
    "# Step 1: take the data (training or testing) and select only the columns of interest\n",
    "# Step 2: transform all of the features to Standard Variables\n",
    "# Step 3: feed the data into a Multi-Layer Perceptron \n",
    "\n",
    "NN_full_feat = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(features)),\n",
    "    ('sdt', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(solver='sgd', activation='logistic', alpha=1.0)) #, alpha=0.1))\n",
    "    ])\n",
    "\n",
    "RF_full_feat = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(features)),\n",
    "    ('sdt', StandardScaler()),\n",
    "    ('rfr', RandomForestRegressor(n_estimators=500))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkDTRN = featdataTRN.copy()\n",
    "junkDTRN = pd.DataFrame(data=junkDTRN, index=None)  ### WHAT THE FUNKKKKKKKKKKKKKK!!!!!!!!!!!!!!!!\n",
    "featdataTRN = junkDTRN.copy()\n",
    "type(featdataTRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26133, 128)\n",
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.889134 -0.759561 -0.179408 -0.323511 -0.337397 -0.165041 -0.350181   \n",
      "1 -0.302847 -0.259215 -0.039553 -0.071390  0.354448  0.034934 -0.074294   \n",
      "2 -0.595990  1.141752 -0.179408 -0.281491 -0.181732 -0.040056 -0.212238   \n",
      "3  0.283441 -0.359285 -0.431148 -0.113410 -0.389286 -0.490001 -0.192531   \n",
      "4  1.456015  2.742858  0.743636  0.895075  0.613890  1.209788  0.516891   \n",
      "\n",
      "        7         8         9      ...          118      119       120  \\\n",
      "0 -0.468994 -0.194901 -0.282010    ...    -0.352879 -0.34047 -0.354456   \n",
      "1  0.256482  0.118815  0.766883    ...    -0.352879 -0.34047 -0.354456   \n",
      "2 -0.087165 -0.473760 -0.362694    ...    -0.352879 -0.34047 -0.354456   \n",
      "3 -0.354446 -0.857191 -0.604746    ...     2.280410 -0.34047 -0.354456   \n",
      "4  0.905592  0.955391  0.887909    ...    -0.352879 -0.34047 -0.354456   \n",
      "\n",
      "        121       122       123       124       125       126       127  \n",
      "0 -0.386464 -0.438399 -0.480146 -0.519751 -0.556029 -0.544055 -0.525775  \n",
      "1 -0.386464 -0.438399  0.829220 -0.519751  0.435152 -0.544055  0.507159  \n",
      "2 -0.386464 -0.438399 -0.480146 -0.519751 -0.556029  0.402473 -0.525775  \n",
      "3 -0.386464 -0.438399 -0.480146  1.621272 -0.556029 -0.544055 -0.525775  \n",
      "4 -0.386464 -0.438399 -0.480146  0.550761 -0.556029  0.402473  2.573026  \n",
      "\n",
      "[5 rows x 128 columns]\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24135</th>\n",
       "      <td>WGPu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16088</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25285</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "24135   WGPu\n",
       "16088   I131\n",
       "25285   I131\n",
       "38496    Bkg\n",
       "4303   Tc99m"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(featdataTRN.shape)\n",
    "print(featdataTRN.head())\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(type(featdataTRN))\n",
    "featkeysTRN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-584-f5370da5824e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-411-36dfdbfc867f>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# return a new array with just the columns you specify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mnewarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnewarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "params =  {'rfr__n_estimators':500, 'rfr__max_features': 50}\n",
    "RF_full_feat.set_params(**params)\n",
    "\n",
    "RF_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred5 = RF_full_feat.predict(featdataTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred4 = NN_subset_feat.predict(featdataTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(y_pred5))\n",
    "len(y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(featdata, featkeys, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featkeysTES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2fde95deabdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatkeysTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'featkeysTES' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(featkeysTES))\n",
    "len(subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN_full_feat.set_params(alpha = 100)\n",
    "NN_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred3 = NN_full_feat.predict(featdataTES)\n",
    "\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "# Save the trained model\n",
    "#joblib.dump(NN_full_feat, '/Users/turk_la/Documents/SSAM/Data/UF6data/NN_full_features.pkl')\n",
    "\n",
    "y_test_feat['pred'] = y_pred3\n",
    "y_test_feat['dif'] = abs(y_test_feat['x'] - y_test_feat['pred'])\n",
    "y_test_feat['dif'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred4 = NN_subset_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred4'] = y_pred4\n",
    "y_test_feat['dif_sub'] = abs(y_test_feat['x'] - y_test_feat['pred4'])\n",
    "y_test_feat['dif_sub'].sum()/len(featkeysTES)\n",
    "\n",
    "# Save the trained model\n",
    "#joblib.dump(NN_subset_feat, '/Users/turk_la/Documents/SSAM/Data/UF6data/NN_sub_features.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Random Forest to do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RF_subset_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-01899e56d702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m{\u001b[0m\u001b[1;34m'rfr__n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rfr__max_features'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m65\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRF_subset_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mRF_subset_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_subset_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RF_subset_feat' is not defined"
     ]
    }
   ],
   "source": [
    "params =  {'rfr__n_estimators':800, 'rfr__max_features': 65}\n",
    "RF_subset_feat.set_params(**params)\n",
    "\n",
    "RF_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred5 = RF_subset_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred5'] = y_pred5\n",
    "y_test_feat['dif_sub_rf'] = abs(y_test_feat['x'] - y_test_feat['pred5'])\n",
    "y_test_feat['dif_sub_rf'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cst': ColumnSelectTransformer(col_names=['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23',...', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128']),\n",
       " 'cst__col_names': ['Channel1',\n",
       "  'Channel2',\n",
       "  'Channel3',\n",
       "  'Channel4',\n",
       "  'Channel5',\n",
       "  'Channel6',\n",
       "  'Channel7',\n",
       "  'Channel8',\n",
       "  'Channel9',\n",
       "  'Channel10',\n",
       "  'Channel11',\n",
       "  'Channel12',\n",
       "  'Channel13',\n",
       "  'Channel14',\n",
       "  'Channel15',\n",
       "  'Channel16',\n",
       "  'Channel17',\n",
       "  'Channel18',\n",
       "  'Channel19',\n",
       "  'Channel20',\n",
       "  'Channel21',\n",
       "  'Channel22',\n",
       "  'Channel23',\n",
       "  'Channel24',\n",
       "  'Channel25',\n",
       "  'Channel26',\n",
       "  'Channel27',\n",
       "  'Channel28',\n",
       "  'Channel29',\n",
       "  'Channel30',\n",
       "  'Channel31',\n",
       "  'Channel32',\n",
       "  'Channel33',\n",
       "  'Channel34',\n",
       "  'Channel35',\n",
       "  'Channel36',\n",
       "  'Channel37',\n",
       "  'Channel38',\n",
       "  'Channel39',\n",
       "  'Channel40',\n",
       "  'Channel41',\n",
       "  'Channel42',\n",
       "  'Channel43',\n",
       "  'Channel44',\n",
       "  'Channel45',\n",
       "  'Channel46',\n",
       "  'Channel47',\n",
       "  'Channel48',\n",
       "  'Channel49',\n",
       "  'Channel50',\n",
       "  'Channel51',\n",
       "  'Channel52',\n",
       "  'Channel53',\n",
       "  'Channel54',\n",
       "  'Channel55',\n",
       "  'Channel56',\n",
       "  'Channel57',\n",
       "  'Channel58',\n",
       "  'Channel59',\n",
       "  'Channel60',\n",
       "  'Channel61',\n",
       "  'Channel62',\n",
       "  'Channel63',\n",
       "  'Channel64',\n",
       "  'Channel65',\n",
       "  'Channel66',\n",
       "  'Channel67',\n",
       "  'Channel68',\n",
       "  'Channel69',\n",
       "  'Channel70',\n",
       "  'Channel71',\n",
       "  'Channel72',\n",
       "  'Channel73',\n",
       "  'Channel74',\n",
       "  'Channel75',\n",
       "  'Channel76',\n",
       "  'Channel77',\n",
       "  'Channel78',\n",
       "  'Channel79',\n",
       "  'Channel80',\n",
       "  'Channel81',\n",
       "  'Channel82',\n",
       "  'Channel83',\n",
       "  'Channel84',\n",
       "  'Channel85',\n",
       "  'Channel86',\n",
       "  'Channel87',\n",
       "  'Channel88',\n",
       "  'Channel89',\n",
       "  'Channel90',\n",
       "  'Channel91',\n",
       "  'Channel92',\n",
       "  'Channel93',\n",
       "  'Channel94',\n",
       "  'Channel95',\n",
       "  'Channel96',\n",
       "  'Channel97',\n",
       "  'Channel98',\n",
       "  'Channel99',\n",
       "  'Channel100',\n",
       "  'Channel101',\n",
       "  'Channel102',\n",
       "  'Channel103',\n",
       "  'Channel104',\n",
       "  'Channel105',\n",
       "  'Channel106',\n",
       "  'Channel107',\n",
       "  'Channel108',\n",
       "  'Channel109',\n",
       "  'Channel110',\n",
       "  'Channel111',\n",
       "  'Channel112',\n",
       "  'Channel113',\n",
       "  'Channel114',\n",
       "  'Channel115',\n",
       "  'Channel116',\n",
       "  'Channel117',\n",
       "  'Channel118',\n",
       "  'Channel119',\n",
       "  'Channel120',\n",
       "  'Channel121',\n",
       "  'Channel122',\n",
       "  'Channel123',\n",
       "  'Channel124',\n",
       "  'Channel125',\n",
       "  'Channel126',\n",
       "  'Channel127',\n",
       "  'Channel128'],\n",
       " 'memory': None,\n",
       " 'rfr': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "            max_features=65, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=800, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       " 'rfr__bootstrap': True,\n",
       " 'rfr__criterion': 'mse',\n",
       " 'rfr__max_depth': None,\n",
       " 'rfr__max_features': 65,\n",
       " 'rfr__max_leaf_nodes': None,\n",
       " 'rfr__min_impurity_decrease': 0.0,\n",
       " 'rfr__min_impurity_split': None,\n",
       " 'rfr__min_samples_leaf': 1,\n",
       " 'rfr__min_samples_split': 2,\n",
       " 'rfr__min_weight_fraction_leaf': 0.0,\n",
       " 'rfr__n_estimators': 800,\n",
       " 'rfr__n_jobs': 1,\n",
       " 'rfr__oob_score': False,\n",
       " 'rfr__random_state': None,\n",
       " 'rfr__verbose': 0,\n",
       " 'rfr__warm_start': False,\n",
       " 'sdt': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'sdt__copy': True,\n",
       " 'sdt__with_mean': True,\n",
       " 'sdt__with_std': True,\n",
       " 'steps': [('cst',\n",
       "   ColumnSelectTransformer(col_names=['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23',...', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128'])),\n",
       "  ('sdt', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('rfr',\n",
       "   RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "              max_features=65, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "              min_impurity_split=None, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=800, n_jobs=1, oob_score=False, random_state=None,\n",
       "              verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_full_feat.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-aba82b107eb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m{\u001b[0m\u001b[1;34m'rfr__n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rfr__max_features'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m365\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-36dfdbfc867f>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# return a new array with just the columns you specify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mnewarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnewarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "#params = ['n_estimators': 800, 'max_features': 165\n",
    "params =  {'rfr__n_estimators':1200, 'rfr__max_features': 365}\n",
    "RF_full_feat.set_params(**params)\n",
    "RF_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred6 = RF_full_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred6'] = y_pred6\n",
    "y_test_feat['dif_sub_rf'] = abs(y_test_feat['x'] - y_test_feat['pred6'])\n",
    "y_test_feat['dif_sub_rf'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred3\n",
    "print(y_pred4.shape)\n",
    "w = sum(y_pred4)\n",
    "print(w/260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## making  submission\n",
    "#submission = pd.read_csv('/home/jack/projects/dengue/data/submission_format.csv')\n",
    "\n",
    "# submission 13 for Iq\n",
    "submission = pd.read_csv('/home/jack/projects/dengue/results/predictions13_EM.csv')\n",
    "\n",
    "count = 0\n",
    "for i in y_pred4:\n",
    "    if i < 0.0:\n",
    "        print(i)\n",
    "        submission.loc[count,'total_cases'] = 0.0\n",
    "    else:\n",
    "        submission.loc[count,'total_cases'] = i\n",
    "    count += 1\n",
    "\n",
    "submission.to_csv('/home/jack/projects/dengue/results/predictions29_EM.csv',index='FALSE')\n",
    "    \n",
    "#submission.total_cases = np.concatenate([y_pred6])\n",
    "    \n",
    "#submission.head()\n",
    "#submission.tail()\n",
    "#submission.shape\n",
    "#submission.loc[411, 'weekofyear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_fullfeat = forest_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Experimenting with implementing a time series aspect\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = featdata[:800], featdata[800:], featkeys[:800], featkeys[800:]\n",
    "\n",
    "RF_full_feat.fit(X_tr,y_tr)\n",
    "\n",
    "preds = []\n",
    "\n",
    "new_val= y_tr['x'][799]\n",
    "\n",
    "for i in range(936-800):\n",
    "\n",
    "    #print(new_val)\n",
    "\n",
    "    X_ts.loc[i+800,'prior_val'] = float(new_val)\n",
    "\n",
    "    #print(X_ts.loc[i+800,'prior_val'])\n",
    "\n",
    "    datum = X_ts[i:i+1]\n",
    "\n",
    "    #print(datum)    \n",
    "\n",
    "    new_val = RF_full_feat.predict(datum)\n",
    "\n",
    "    preds.append(float(new_val))\n",
    "\n",
    "    \n",
    "\n",
    "y_ts['pred'] = preds\n",
    "\n",
    "y_ts['dif'] = abs(y_ts['x'] - y_ts['pred'])\n",
    "\n",
    "y_ts['dif'].sum()/len(y_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
