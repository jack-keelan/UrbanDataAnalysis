{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Selection for UF6 Neural Network\n",
    "This notebook will explore the value of feature engineering and selection for the UF6 GADRAS data set.\n",
    "We will explore the predictor correlations, down-select predictors based on correlation, cross-validate to choose model hyperparameters and finally train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all of the libraries \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import base\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copied from the http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "# This defines a function to build a confusion matrix, which we will use later.\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "   # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def forest_CV(columns, X_train, X_test, y_train, y_test):\n",
    "#    n_est = [800]\n",
    "    n_est = [10,20,30,40,50]\n",
    "#    max_feat = [31, 61, 91, 121, 151, 181, 230, 400]\n",
    "#    max_feat = [56, 61, 66]\n",
    "    max_feat = [10,20,30,40,50]\n",
    "    min_samp_split = [2]\n",
    "    min_samp_leaf = [1]\n",
    "    names = []\n",
    "    classifiers = []\n",
    "    for i in n_est:\n",
    "        for j in max_feat:\n",
    "            for k in min_samp_split:\n",
    "                for l in min_samp_leaf:\n",
    "                    names.append('n_est='+str(i)+' max_feat=' +str(j) + ' min_samp_split=' +str(k) +' min_samp_leaf=' +str(l))\n",
    "                    classifiers.append(RandomForestRegressor(n_estimators=i, max_features=j, min_samples_split=k,\n",
    "                                                min_samples_leaf=l))\n",
    "    # iterate over classifiers\n",
    "    y = []\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        score = clf.score(X_test, y_test.values.ravel())\n",
    "        y.append(score)\n",
    "        print(name, score)\n",
    "    return(zip(names, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this class will allow us to select different features, which are columns in the data set\n",
    "# it is necessary to create a Class of this type for use in Pipelines\n",
    "\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # return a new array with just the columns you specify\n",
    "        newarray = X.filter(self.col_names, axis=1)\n",
    "        return newarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data_df = pd.read_csv('/Users/mooreet_la/projects/SDRD/competition/data/TNG_set.csv')\n",
    "labels = data_df['SourceID'].copy()\n",
    "locations = data_df['location'].copy()\n",
    "channel_data = data_df.iloc[:, 0:128].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23', 'Channel24', 'Channel25', 'Channel26', 'Channel27', 'Channel28', 'Channel29', 'Channel30', 'Channel31', 'Channel32', 'Channel33', 'Channel34', 'Channel35', 'Channel36', 'Channel37', 'Channel38', 'Channel39', 'Channel40', 'Channel41', 'Channel42', 'Channel43', 'Channel44', 'Channel45', 'Channel46', 'Channel47', 'Channel48', 'Channel49', 'Channel50', 'Channel51', 'Channel52', 'Channel53', 'Channel54', 'Channel55', 'Channel56', 'Channel57', 'Channel58', 'Channel59', 'Channel60', 'Channel61', 'Channel62', 'Channel63', 'Channel64', 'Channel65', 'Channel66', 'Channel67', 'Channel68', 'Channel69', 'Channel70', 'Channel71', 'Channel72', 'Channel73', 'Channel74', 'Channel75', 'Channel76', 'Channel77', 'Channel78', 'Channel79', 'Channel80', 'Channel81', 'Channel82', 'Channel83', 'Channel84', 'Channel85', 'Channel86', 'Channel87', 'Channel88', 'Channel89', 'Channel90', 'Channel91', 'Channel92', 'Channel93', 'Channel94', 'Channel95', 'Channel96', 'Channel97', 'Channel98', 'Channel99', 'Channel100', 'Channel101', 'Channel102', 'Channel103', 'Channel104', 'Channel105', 'Channel106', 'Channel107', 'Channel108', 'Channel109', 'Channel110', 'Channel111', 'Channel112', 'Channel113', 'Channel114', 'Channel115', 'Channel116', 'Channel117', 'Channel118', 'Channel119', 'Channel120', 'Channel121', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128']\n",
      "128\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# define the set of predictors you want to use to train the model\n",
    "\n",
    "features = list(channel_data) # takes the engineered features\n",
    "#features.remove('city')\n",
    "cst = ColumnSelectTransformer(features)\n",
    "\n",
    "print(features)\n",
    "print(len(features))\n",
    "print(type(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.0\n",
      "1    0.0\n",
      "2    0.0\n",
      "3    0.0\n",
      "4    0.0\n",
      "Name: location, dtype: float64\n",
      "(39005,)\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: SourceID, dtype: int64\n",
      "   Channel1  Channel2  Channel3  Channel4  Channel5  Channel6  Channel7  \\\n",
      "0         5        28        90       150       134       141        89   \n",
      "1         4        30        98       171       139       102       107   \n",
      "2         7        31       106       165       134       131       115   \n",
      "3         3        34       108       179       142       129        85   \n",
      "4        10        31        96       162       170        98       118   \n",
      "\n",
      "   Channel8  Channel9  Channel10     ...      Channel119  Channel120  \\\n",
      "0        71        66         59     ...               0           0   \n",
      "1        99        65         62     ...               0           1   \n",
      "2        84        70         57     ...               0           0   \n",
      "3        85        68         65     ...               0           0   \n",
      "4        93        63         53     ...               0           1   \n",
      "\n",
      "   Channel121  Channel122  Channel123  Channel124  Channel125  Channel126  \\\n",
      "0           1           1           0           0           0           1   \n",
      "1           0           0           0           1           0           0   \n",
      "2           0           0           1           0           0           1   \n",
      "3           0           0           1           0           0           1   \n",
      "4           0           0           1           1           0           0   \n",
      "\n",
      "   Channel127  Channel128  \n",
      "0           3           2  \n",
      "1           1           0  \n",
      "2           2           0  \n",
      "3           3           1  \n",
      "4           0           1  \n",
      "\n",
      "[5 rows x 128 columns]\n",
      "(39005, 128)\n"
     ]
    }
   ],
   "source": [
    "print(locations.head())\n",
    "print(locations.shape)\n",
    "print(labels.head())\n",
    "print(channel_data.head())\n",
    "print(channel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-0.226700</td>\n",
       "      <td>-0.576024</td>\n",
       "      <td>0.184333</td>\n",
       "      <td>0.056506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.131533</td>\n",
       "      <td>-0.195570</td>\n",
       "      <td>-0.514837</td>\n",
       "      <td>-0.903331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>-1.066853</td>\n",
       "      <td>-1.510185</td>\n",
       "      <td>1.727097</td>\n",
       "      <td>0.851405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-1.305350</td>\n",
       "      <td>0.057807</td>\n",
       "      <td>0.961729</td>\n",
       "      <td>0.411971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>-1.239719</td>\n",
       "      <td>-0.788961</td>\n",
       "      <td>-1.093165</td>\n",
       "      <td>-1.459578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.236398</td>\n",
       "      <td>-0.306419</td>\n",
       "      <td>0.300387</td>\n",
       "      <td>1.437147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "a -0.226700 -0.576024  0.184333  0.056506\n",
       "b  0.131533 -0.195570 -0.514837 -0.903331\n",
       "c -1.066853 -1.510185  1.727097  0.851405\n",
       "d -1.305350  0.057807  0.961729  0.411971\n",
       "e -1.239719 -0.788961 -1.093165 -1.459578\n",
       "f  0.236398 -0.306419  0.300387  1.437147"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(np.random.randn(6,4),\n",
    "               index=list('abcdef'),                 columns=list('ABCD'))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-1.305350</td>\n",
       "      <td>0.057807</td>\n",
       "      <td>0.961729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>-1.239719</td>\n",
       "      <td>-0.788961</td>\n",
       "      <td>-1.093165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.236398</td>\n",
       "      <td>-0.306419</td>\n",
       "      <td>0.300387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "d -1.305350  0.057807  0.961729\n",
       "e -1.239719 -0.788961 -1.093165\n",
       "f  0.236398 -0.306419  0.300387"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = df1.loc['d':, 'A':'C']\n",
    "f\n",
    "#g = df1.iloc[3:,0:3]\n",
    "#g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "4    0\n",
      "Name: SourceID, dtype: int64\n",
      "   SourceID\n",
      "0         0\n",
      "1         0\n",
      "2         0\n",
      "3         0\n",
      "4         0\n"
     ]
    }
   ],
   "source": [
    "junk = labels.copy()\n",
    "hasattr(labels,'filter')\n",
    "print(type(labels))\n",
    "print(type(junk))\n",
    "junk = pd.DataFrame(data=junk, index=None)\n",
    "print(type(junk))\n",
    "print(labels.head())\n",
    "print(junk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labelsDFtest = labels.copy()\n",
    "# print(labelsDFtest.shape)\n",
    "# print(labelsDFtest.head())\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 0, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 1, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 2, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 3, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 4, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 5, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 6, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# print(labelsDFtest.shape)\n",
    "# print(labelsDFtest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(labelsDFtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Bkg\n",
      "1    Bkg\n",
      "2    Bkg\n",
      "3    Bkg\n",
      "4    Bkg\n",
      "Name: SourceID, dtype: object\n",
      "(39005,)\n",
      "<class 'numpy.ndarray'>\n",
      "(39005, 1)\n"
     ]
    }
   ],
   "source": [
    "#featkeys = channel_data['SourceIDs']\n",
    "#featkeys = np.reshape(featkeys, (29000,1))\n",
    "\n",
    "#featdata = featdata.drop('SourceIDs', axis=1)\n",
    "# print(featdata.shape)\n",
    "# print(featkeys.shape)\n",
    "\n",
    "labels = labels.replace(to_replace= 0, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 1, value='HEU', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 2, value='WGPu', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 3, value='I131', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 4, value='Co60', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 5, value='Tc99m', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 6, value='Tc+HEU', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "\n",
    "# for i in range(0,labels.shape[0]):\n",
    "#     if labels[i] == 0:\n",
    "#         labels[i] = 'Bkg'\n",
    "#     elif labels[i] == 1:\n",
    "#         labels[i] = 'HEU'\n",
    "#     elif labels[i] == 2:\n",
    "#         labels[i] = 'WGPu'\n",
    "#     elif labels[i] == 3:\n",
    "#         labels[i] = 'I131'\n",
    "#     elif labels[i] == 4:\n",
    "#         labels[i] = 'Co60'\n",
    "#     elif labels[i] == 5:\n",
    "#         labels[i] = 'Tc99m'\n",
    "#     else:\n",
    "#         labels[i] = 'Tc+HEU'\n",
    "        \n",
    "print(labels.head())\n",
    "print(labels.shape)\n",
    "labels = labels.values.reshape([labels.shape[0],1])\n",
    "print(type(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 1)\n",
      "(39005, 128)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "junk = labels.copy()\n",
    "junk = pd.DataFrame(data=junk, index=None)\n",
    "featkeys = junk.copy()\n",
    "featdata = channel_data.copy()\n",
    "\n",
    "print(featkeys.shape)\n",
    "print(featdata.shape)\n",
    "print(type(labels))\n",
    "print(type(featkeys))\n",
    "print(type(featdata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will perform cross validations to select hyperparameters for four different models: \n",
    "full channel, subset channel, full feature, subset feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess dataset, split into training and test part\n",
    "def data_split(data, keys, columns):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, keys, \n",
    "                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(X_train)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(X_test)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transform(dataTRN, dataTEST, columns):\n",
    "    # just transform the already split data    ### This is redundant if you already used the split command!!\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(dataTRN)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(dataTEST)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat = data_split(featdata, featkeys, features)\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat = data_split(featdata, featkeys, subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26133, 128)\n",
      "(12872, 128)\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "(26133, 128)\n",
      "(12872, 128)\n",
      "(39005, 128)\n",
      "(26133, 1)\n",
      "(12872, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_fullfeat.shape)\n",
    "print(X_test_fullfeat.shape)\n",
    "print(y_train_fullfeat.shape)\n",
    "print(y_test_fullfeat.shape)\n",
    "featdataTRN = X_train_fullfeat.copy()\n",
    "featdataTES = X_test_fullfeat.copy()\n",
    "featkeysTRN = y_train_fullfeat.copy()\n",
    "featkeysTES = y_test_fullfeat.copy()\n",
    "print(featdataTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featdata.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12872, 1)\n",
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(featkeysTES.shape)\n",
    "print(type(featkeysTES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  only if not splitting ????\n",
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "#y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat = data_transform(featdataTRN, featdataTES, subset_features)\n",
    "#y_train_subfeat, y_test_subfeat = featkeysTRN, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "(12872, 1)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#featkeysTES = 0\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "(26133, 128)\n",
      "(12872, 128)\n"
     ]
    }
   ],
   "source": [
    "# take a quick look, these should have the same number of rows\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "#print(featkeys1.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featdataTES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use the split data with the PCR & PLS in the dataframe\n",
    "\n",
    "#featdata = featdataTRN\n",
    "#featkeys = featkeysTRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1</th>\n",
       "      <th>Channel2</th>\n",
       "      <th>Channel3</th>\n",
       "      <th>Channel4</th>\n",
       "      <th>Channel5</th>\n",
       "      <th>Channel6</th>\n",
       "      <th>Channel7</th>\n",
       "      <th>Channel8</th>\n",
       "      <th>Channel9</th>\n",
       "      <th>Channel10</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel119</th>\n",
       "      <th>Channel120</th>\n",
       "      <th>Channel121</th>\n",
       "      <th>Channel122</th>\n",
       "      <th>Channel123</th>\n",
       "      <th>Channel124</th>\n",
       "      <th>Channel125</th>\n",
       "      <th>Channel126</th>\n",
       "      <th>Channel127</th>\n",
       "      <th>Channel128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>134</td>\n",
       "      <td>141</td>\n",
       "      <td>89</td>\n",
       "      <td>71</td>\n",
       "      <td>66</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>98</td>\n",
       "      <td>171</td>\n",
       "      <td>139</td>\n",
       "      <td>102</td>\n",
       "      <td>107</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>106</td>\n",
       "      <td>165</td>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>115</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>108</td>\n",
       "      <td>179</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>68</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>96</td>\n",
       "      <td>162</td>\n",
       "      <td>170</td>\n",
       "      <td>98</td>\n",
       "      <td>118</td>\n",
       "      <td>93</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel1  Channel2  Channel3  Channel4  Channel5  Channel6  Channel7  \\\n",
       "0         5        28        90       150       134       141        89   \n",
       "1         4        30        98       171       139       102       107   \n",
       "2         7        31       106       165       134       131       115   \n",
       "3         3        34       108       179       142       129        85   \n",
       "4        10        31        96       162       170        98       118   \n",
       "\n",
       "   Channel8  Channel9  Channel10     ...      Channel119  Channel120  \\\n",
       "0        71        66         59     ...               0           0   \n",
       "1        99        65         62     ...               0           1   \n",
       "2        84        70         57     ...               0           0   \n",
       "3        85        68         65     ...               0           0   \n",
       "4        93        63         53     ...               0           1   \n",
       "\n",
       "   Channel121  Channel122  Channel123  Channel124  Channel125  Channel126  \\\n",
       "0           1           1           0           0           0           1   \n",
       "1           0           0           0           1           0           0   \n",
       "2           0           0           1           0           0           1   \n",
       "3           0           0           1           0           0           1   \n",
       "4           0           0           1           1           0           0   \n",
       "\n",
       "   Channel127  Channel128  \n",
       "0           3           2  \n",
       "1           1           0  \n",
       "2           2           0  \n",
       "3           3           1  \n",
       "4           0           1  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39005, 1)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featkeys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will perform cross validations to select hyperparameters for different models: \n",
    "full 128 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best value of alpha\n",
    "\n",
    "def alpha_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    alphas = np.logspace(-5, 3, 9)\n",
    "    names = []\n",
    "    for i in alphas:\n",
    "        names.append('alpha ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in alphas:\n",
    "        classifiers.append(MLPRegressor(solver='adam', activation='tanh', alpha=i, random_state=1))\n",
    "\n",
    "    x = alphas\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "    \n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(alphas, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best solver\n",
    "\n",
    "def solver_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    solvers = ['lbfgs','sgd','adam']\n",
    "    names = []\n",
    "    for i in solvers:\n",
    "        names.append('solver= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in solvers:\n",
    "        classifiers.append(MLPRegressor(solver=i, activation='tanh', learning_rate='adaptive', alpha=0.1, random_state=1))\n",
    "        \n",
    "    x = [1,2,3]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(solvers, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def activation_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    activators = ['identity', 'logistic', 'tanh', 'relu']\n",
    "    names = []\n",
    "    for i in activators:\n",
    "        names.append('activator= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in activators:\n",
    "        classifiers.append(MLPRegressor(solver='adam', activation=i, alpha=0.1, random_state=1))\n",
    "\n",
    "    x = [1,2,3,4]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(activators, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess dataset, split into training and test part\n",
    "def data_split(data, keys, columns):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, keys, \n",
    "                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(X_train)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(X_test)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std, y_train, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transform(dataTRN, dataTEST, columns):\n",
    "    # just transform the already split data\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(dataTRN)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(dataTEST)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat = data_split(featdata, featkeys, features)\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat = data_split(featdata, featkeys, subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "(12872, 1)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#featkeysTES = 0\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "#y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat = data_transform(featdataTRN, featdataTES, subset_features)\n",
    "#y_train_subfeat, y_test_subfeat = featkeysTRN, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31438</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24333</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31428</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29359</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36466</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34893</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21241</th>\n",
       "      <td>Co60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9738</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4903</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3245</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13370</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20979</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35578</th>\n",
       "      <td>Co60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36165</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28962</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34117</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35033</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15639</th>\n",
       "      <td>Co60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24308</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17291</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>953</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10843</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38389</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32394</th>\n",
       "      <td>WGPu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27316</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2624</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21089</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25361</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35651</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38770</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26227</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13984</th>\n",
       "      <td>WGPu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32096</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13135</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34886</th>\n",
       "      <td>Co60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3686</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36294</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24606</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35722</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1584</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6453</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11747</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15306</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21551</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37055</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36662</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37646</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25707</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37185</th>\n",
       "      <td>Co60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14192</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19263</th>\n",
       "      <td>Tc+HEU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1765</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24525</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30615</th>\n",
       "      <td>HEU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12872 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "31438     Bkg\n",
       "24333   Tc99m\n",
       "31428     Bkg\n",
       "29359  Tc+HEU\n",
       "36466   Tc99m\n",
       "34893     HEU\n",
       "21241    Co60\n",
       "9738   Tc+HEU\n",
       "4903      HEU\n",
       "3245   Tc+HEU\n",
       "7718   Tc+HEU\n",
       "13370     HEU\n",
       "20979     Bkg\n",
       "35578    Co60\n",
       "36165     Bkg\n",
       "28962    I131\n",
       "7613      Bkg\n",
       "34117     Bkg\n",
       "35033     Bkg\n",
       "15639    Co60\n",
       "24308     Bkg\n",
       "17291   Tc99m\n",
       "953    Tc+HEU\n",
       "10843     Bkg\n",
       "323       Bkg\n",
       "38389    I131\n",
       "32394    WGPu\n",
       "27316     Bkg\n",
       "2624    Tc99m\n",
       "21089     Bkg\n",
       "...       ...\n",
       "25361     Bkg\n",
       "35651     Bkg\n",
       "38770    I131\n",
       "26227     HEU\n",
       "13984    WGPu\n",
       "32096     HEU\n",
       "13135     Bkg\n",
       "34886    Co60\n",
       "3686     I131\n",
       "291       Bkg\n",
       "7720      HEU\n",
       "36294     Bkg\n",
       "24606     Bkg\n",
       "2085      HEU\n",
       "35722     Bkg\n",
       "1584   Tc+HEU\n",
       "6453   Tc+HEU\n",
       "11747     HEU\n",
       "15306     Bkg\n",
       "21551   Tc99m\n",
       "37055     Bkg\n",
       "36662     Bkg\n",
       "37646  Tc+HEU\n",
       "25707  Tc+HEU\n",
       "37185    Co60\n",
       "14192     Bkg\n",
       "19263  Tc+HEU\n",
       "1765      Bkg\n",
       "24525   Tc99m\n",
       "30615     HEU\n",
       "\n",
       "[12872 rows x 1 columns]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'I131'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-ad3ede166f2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malpha_full_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_fullfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha_full_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msolver_full_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_fullfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver_full_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-e85e91523e1c>\u001b[0m in \u001b[0;36malpha_CV\u001b[1;34m(columns, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# iterate over classifiers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    329\u001b[0m                              hidden_layer_sizes)\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m   1302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m-> 1304\u001b[1;33m                          multi_output=True, y_numeric=True)\n\u001b[0m\u001b[0;32m   1305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'I131'"
     ]
    }
   ],
   "source": [
    "alpha_full_feat = alpha_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat)\n",
    "print(list(alpha_full_feat))\n",
    "\n",
    "solver_full_feat = solver_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat)\n",
    "print(list(solver_full_feat))\n",
    "\n",
    "activation_full_feat = activation_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat)\n",
    "print(list(activation_full_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_sub_feat = alpha_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat)\n",
    "print(list(alpha_sub_feat))\n",
    "\n",
    "solver_sub_feat = solver_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat)\n",
    "print(list(solver_sub_feat))\n",
    "\n",
    "activation_sub_feat = activation_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat)\n",
    "print(list(activation_sub_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finallly we will train four different models on the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just read in data from files and over-write\n",
    "# featkeysTRN = pd.read_csv('/home/jack/projects/dengue/data/sj_train100per_labels.csv')\n",
    "\n",
    "# featdataTRN = pd.read_csv('/home/jack/projects/dengue/data/sj_train100per.csv')\n",
    "# featdataTES = pd.read_csv('/home/jack/projects/dengue/data/sj_testTrue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'numpy.ndarray'>\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(type(featdata))\n",
    "print(type(featkeys))\n",
    "print(type(featdataTRN))\n",
    "print(type(featkeysTRN))\n",
    "print(type(featdataTES))\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline which you can use to train and predict\n",
    "# Step 1: take the data (training or testing) and select only the columns of interest\n",
    "# Step 2: transform all of the features to Standard Variables\n",
    "# Step 3: feed the data into a Multi-Layer Perceptron \n",
    "\n",
    "NN_full_feat = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(features)),\n",
    "    ('sdt', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(solver='adam', activation='relu', alpha=100.0)) #, alpha=0.1))\n",
    "    ])\n",
    "\n",
    "RF_full_feat = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(features)),\n",
    "    ('sdt', StandardScaler()),\n",
    "    ('rfr', RandomForestRegressor(n_estimators=800))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "junkDTRN = featdataTRN.copy()\n",
    "junkDTRN = pd.DataFrame(data=junkDTRN, index=features)  ### WHAT THE FUNKKKKKKKKKKKKKK!!!!!!!!!!!!!!!!\n",
    "featdataTRN = junkDTRN.copy()\n",
    "type(featdataTRN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n",
      "          0    1    2    3    4    5    6    7    8    9   ...   118  119  \\\n",
      "Channel1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN   \n",
      "Channel2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN   \n",
      "Channel3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN   \n",
      "Channel4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN   \n",
      "Channel5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN ...   NaN  NaN   \n",
      "\n",
      "          120  121  122  123  124  125  126  127  \n",
      "Channel1  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "Channel2  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "Channel3  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "Channel4  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "Channel5  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  \n",
      "\n",
      "[5 rows x 128 columns]\n",
      "(26133, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24135</th>\n",
       "      <td>WGPu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16088</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25285</th>\n",
       "      <td>I131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>Bkg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4303</th>\n",
       "      <td>Tc99m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "24135   WGPu\n",
       "16088   I131\n",
       "25285   I131\n",
       "38496    Bkg\n",
       "4303   Tc99m"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(featdataTRN.shape)\n",
    "print(featdataTRN.head())\n",
    "print(featkeysTRN.shape)\n",
    "featkeysTRN.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(26133, 0)) while a minimum of 1 is required by StandardScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-155-56c28575a3fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    610\u001b[0m         \"\"\"\n\u001b[0;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m--> 612\u001b[1;33m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m         \u001b[1;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    468\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 470\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(26133, 0)) while a minimum of 1 is required by StandardScaler."
     ]
    }
   ],
   "source": [
    "params =  {'rfr__n_estimators':800, 'rfr__max_features': 65}\n",
    "RF_full_feat.set_params(**params)\n",
    "\n",
    "RF_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred5 = RF_full_feat.predict(featdataTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred4 = NN_subset_feat.predict(featdataTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(y_pred5))\n",
    "len(y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(featdata, featkeys, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featkeysTES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2fde95deabdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatkeysTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'featkeysTES' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(featkeysTES))\n",
    "len(subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN_full_feat.set_params(alpha = 100)\n",
    "NN_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred3 = NN_full_feat.predict(featdataTES)\n",
    "\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "# Save the trained model\n",
    "#joblib.dump(NN_full_feat, '/Users/turk_la/Documents/SSAM/Data/UF6data/NN_full_features.pkl')\n",
    "\n",
    "y_test_feat['pred'] = y_pred3\n",
    "y_test_feat['dif'] = abs(y_test_feat['x'] - y_test_feat['pred'])\n",
    "y_test_feat['dif'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred4 = NN_subset_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred4'] = y_pred4\n",
    "y_test_feat['dif_sub'] = abs(y_test_feat['x'] - y_test_feat['pred4'])\n",
    "y_test_feat['dif_sub'].sum()/len(featkeysTES)\n",
    "\n",
    "# Save the trained model\n",
    "#joblib.dump(NN_subset_feat, '/Users/turk_la/Documents/SSAM/Data/UF6data/NN_sub_features.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Random Forest to do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RF_subset_feat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-01899e56d702>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m{\u001b[0m\u001b[1;34m'rfr__n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m800\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rfr__max_features'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m65\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRF_subset_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mRF_subset_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_subset_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RF_subset_feat' is not defined"
     ]
    }
   ],
   "source": [
    "params =  {'rfr__n_estimators':800, 'rfr__max_features': 65}\n",
    "RF_subset_feat.set_params(**params)\n",
    "\n",
    "RF_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred5 = RF_subset_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred5'] = y_pred5\n",
    "y_test_feat['dif_sub_rf'] = abs(y_test_feat['x'] - y_test_feat['pred5'])\n",
    "y_test_feat['dif_sub_rf'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cst': ColumnSelectTransformer(col_names=['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23',...', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128']),\n",
       " 'cst__col_names': ['Channel1',\n",
       "  'Channel2',\n",
       "  'Channel3',\n",
       "  'Channel4',\n",
       "  'Channel5',\n",
       "  'Channel6',\n",
       "  'Channel7',\n",
       "  'Channel8',\n",
       "  'Channel9',\n",
       "  'Channel10',\n",
       "  'Channel11',\n",
       "  'Channel12',\n",
       "  'Channel13',\n",
       "  'Channel14',\n",
       "  'Channel15',\n",
       "  'Channel16',\n",
       "  'Channel17',\n",
       "  'Channel18',\n",
       "  'Channel19',\n",
       "  'Channel20',\n",
       "  'Channel21',\n",
       "  'Channel22',\n",
       "  'Channel23',\n",
       "  'Channel24',\n",
       "  'Channel25',\n",
       "  'Channel26',\n",
       "  'Channel27',\n",
       "  'Channel28',\n",
       "  'Channel29',\n",
       "  'Channel30',\n",
       "  'Channel31',\n",
       "  'Channel32',\n",
       "  'Channel33',\n",
       "  'Channel34',\n",
       "  'Channel35',\n",
       "  'Channel36',\n",
       "  'Channel37',\n",
       "  'Channel38',\n",
       "  'Channel39',\n",
       "  'Channel40',\n",
       "  'Channel41',\n",
       "  'Channel42',\n",
       "  'Channel43',\n",
       "  'Channel44',\n",
       "  'Channel45',\n",
       "  'Channel46',\n",
       "  'Channel47',\n",
       "  'Channel48',\n",
       "  'Channel49',\n",
       "  'Channel50',\n",
       "  'Channel51',\n",
       "  'Channel52',\n",
       "  'Channel53',\n",
       "  'Channel54',\n",
       "  'Channel55',\n",
       "  'Channel56',\n",
       "  'Channel57',\n",
       "  'Channel58',\n",
       "  'Channel59',\n",
       "  'Channel60',\n",
       "  'Channel61',\n",
       "  'Channel62',\n",
       "  'Channel63',\n",
       "  'Channel64',\n",
       "  'Channel65',\n",
       "  'Channel66',\n",
       "  'Channel67',\n",
       "  'Channel68',\n",
       "  'Channel69',\n",
       "  'Channel70',\n",
       "  'Channel71',\n",
       "  'Channel72',\n",
       "  'Channel73',\n",
       "  'Channel74',\n",
       "  'Channel75',\n",
       "  'Channel76',\n",
       "  'Channel77',\n",
       "  'Channel78',\n",
       "  'Channel79',\n",
       "  'Channel80',\n",
       "  'Channel81',\n",
       "  'Channel82',\n",
       "  'Channel83',\n",
       "  'Channel84',\n",
       "  'Channel85',\n",
       "  'Channel86',\n",
       "  'Channel87',\n",
       "  'Channel88',\n",
       "  'Channel89',\n",
       "  'Channel90',\n",
       "  'Channel91',\n",
       "  'Channel92',\n",
       "  'Channel93',\n",
       "  'Channel94',\n",
       "  'Channel95',\n",
       "  'Channel96',\n",
       "  'Channel97',\n",
       "  'Channel98',\n",
       "  'Channel99',\n",
       "  'Channel100',\n",
       "  'Channel101',\n",
       "  'Channel102',\n",
       "  'Channel103',\n",
       "  'Channel104',\n",
       "  'Channel105',\n",
       "  'Channel106',\n",
       "  'Channel107',\n",
       "  'Channel108',\n",
       "  'Channel109',\n",
       "  'Channel110',\n",
       "  'Channel111',\n",
       "  'Channel112',\n",
       "  'Channel113',\n",
       "  'Channel114',\n",
       "  'Channel115',\n",
       "  'Channel116',\n",
       "  'Channel117',\n",
       "  'Channel118',\n",
       "  'Channel119',\n",
       "  'Channel120',\n",
       "  'Channel121',\n",
       "  'Channel122',\n",
       "  'Channel123',\n",
       "  'Channel124',\n",
       "  'Channel125',\n",
       "  'Channel126',\n",
       "  'Channel127',\n",
       "  'Channel128'],\n",
       " 'memory': None,\n",
       " 'rfr': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "            max_features=65, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=800, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       " 'rfr__bootstrap': True,\n",
       " 'rfr__criterion': 'mse',\n",
       " 'rfr__max_depth': None,\n",
       " 'rfr__max_features': 65,\n",
       " 'rfr__max_leaf_nodes': None,\n",
       " 'rfr__min_impurity_decrease': 0.0,\n",
       " 'rfr__min_impurity_split': None,\n",
       " 'rfr__min_samples_leaf': 1,\n",
       " 'rfr__min_samples_split': 2,\n",
       " 'rfr__min_weight_fraction_leaf': 0.0,\n",
       " 'rfr__n_estimators': 800,\n",
       " 'rfr__n_jobs': 1,\n",
       " 'rfr__oob_score': False,\n",
       " 'rfr__random_state': None,\n",
       " 'rfr__verbose': 0,\n",
       " 'rfr__warm_start': False,\n",
       " 'sdt': StandardScaler(copy=True, with_mean=True, with_std=True),\n",
       " 'sdt__copy': True,\n",
       " 'sdt__with_mean': True,\n",
       " 'sdt__with_std': True,\n",
       " 'steps': [('cst',\n",
       "   ColumnSelectTransformer(col_names=['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23',...', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128'])),\n",
       "  ('sdt', StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "  ('rfr',\n",
       "   RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "              max_features=65, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "              min_impurity_split=None, min_samples_leaf=1,\n",
       "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "              n_estimators=800, n_jobs=1, oob_score=False, random_state=None,\n",
       "              verbose=0, warm_start=False))]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF_full_feat.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-aba82b107eb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m{\u001b[0m\u001b[1;34m'rfr__n_estimators'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rfr__max_features'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m365\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-36dfdbfc867f>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# return a new array with just the columns you specify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mnewarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnewarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "#params = ['n_estimators': 800, 'max_features': 165\n",
    "params =  {'rfr__n_estimators':1200, 'rfr__max_features': 365}\n",
    "RF_full_feat.set_params(**params)\n",
    "RF_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred6 = RF_full_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred6'] = y_pred6\n",
    "y_test_feat['dif_sub_rf'] = abs(y_test_feat['x'] - y_test_feat['pred6'])\n",
    "y_test_feat['dif_sub_rf'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred3\n",
    "print(y_pred4.shape)\n",
    "w = sum(y_pred4)\n",
    "print(w/260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## making  submission\n",
    "#submission = pd.read_csv('/home/jack/projects/dengue/data/submission_format.csv')\n",
    "\n",
    "# submission 13 for Iq\n",
    "submission = pd.read_csv('/home/jack/projects/dengue/results/predictions13_EM.csv')\n",
    "\n",
    "count = 0\n",
    "for i in y_pred4:\n",
    "    if i < 0.0:\n",
    "        print(i)\n",
    "        submission.loc[count,'total_cases'] = 0.0\n",
    "    else:\n",
    "        submission.loc[count,'total_cases'] = i\n",
    "    count += 1\n",
    "\n",
    "submission.to_csv('/home/jack/projects/dengue/results/predictions29_EM.csv',index='FALSE')\n",
    "    \n",
    "#submission.total_cases = np.concatenate([y_pred6])\n",
    "    \n",
    "#submission.head()\n",
    "#submission.tail()\n",
    "#submission.shape\n",
    "#submission.loc[411, 'weekofyear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(col_subfeat), X_train_subfeat.shape, X_test_subfeat.shape, y_train_subfeat.shape, y_test_subfeat.shape)\n",
    "y_test_subfeat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_sub_feat = forest_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_fullfeat = forest_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Experimenting with implementing a time series aspect\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = featdata[:800], featdata[800:], featkeys[:800], featkeys[800:]\n",
    "\n",
    "RF_full_feat.fit(X_tr,y_tr)\n",
    "\n",
    "preds = []\n",
    "\n",
    "new_val= y_tr['x'][799]\n",
    "\n",
    "for i in range(936-800):\n",
    "\n",
    "    #print(new_val)\n",
    "\n",
    "    X_ts.loc[i+800,'prior_val'] = float(new_val)\n",
    "\n",
    "    #print(X_ts.loc[i+800,'prior_val'])\n",
    "\n",
    "    datum = X_ts[i:i+1]\n",
    "\n",
    "    #print(datum)    \n",
    "\n",
    "    new_val = RF_full_feat.predict(datum)\n",
    "\n",
    "    preds.append(float(new_val))\n",
    "\n",
    "    \n",
    "\n",
    "y_ts['pred'] = preds\n",
    "\n",
    "y_ts['dif'] = abs(y_ts['x'] - y_ts['pred'])\n",
    "\n",
    "y_ts['dif'].sum()/len(y_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
