{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Selection for UF6 Neural Network\n",
    "This notebook will explore the value of feature engineering and selection for the UF6 GADRAS data set.\n",
    "We will explore the predictor correlations, down-select predictors based on correlation, cross-validate to choose model hyperparameters and finally train models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all of the libraries \n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import base\n",
    "from sklearn.pipeline import Pipeline \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# copied from the http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "# This defines a function to build a confusion matrix, which we will use later.\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "   # print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def forest_CV(columns, X_train, X_test, y_train, y_test):\n",
    "#    n_est = [800]\n",
    "    n_est = [10,20,30,40,50]\n",
    "#    max_feat = [31, 61, 91, 121, 151, 181, 230, 400]\n",
    "#    max_feat = [56, 61, 66]\n",
    "    max_feat = [10,20,30,40,50]\n",
    "    min_samp_split = [2]\n",
    "    min_samp_leaf = [1]\n",
    "    names = []\n",
    "    classifiers = []\n",
    "    for i in n_est:\n",
    "        for j in max_feat:\n",
    "            for k in min_samp_split:\n",
    "                for l in min_samp_leaf:\n",
    "                    names.append('n_est='+str(i)+' max_feat=' +str(j) + ' min_samp_split=' +str(k) +' min_samp_leaf=' +str(l))\n",
    "                    classifiers.append(RandomForestRegressor(n_estimators=i, max_features=j, min_samples_split=k,\n",
    "                                                min_samples_leaf=l))\n",
    "    # iterate over classifiers\n",
    "    y = []\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train.values.ravel())\n",
    "        score = clf.score(X_test, y_test.values.ravel())\n",
    "        y.append(score)\n",
    "        print(name, score)\n",
    "    return(zip(names, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this class will allow us to select different features, which are columns in the data set\n",
    "# it is necessary to create a Class of this type for use in Pipelines\n",
    "\n",
    "class ColumnSelectTransformer(base.BaseEstimator, base.TransformerMixin):\n",
    "    \n",
    "    def __init__(self, col_names):\n",
    "        self.col_names = col_names  # We will need these in transform()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # This transformer doesn't need to learn anything about the data,\n",
    "        # so it can just return self without any further processing\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # return a new array with just the columns you specify\n",
    "        newarray = X.filter(self.col_names, axis=1)\n",
    "        return newarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import data\n",
    "data_df = pd.read_csv('/Users/mooreet_la/projects/SDRD/competition/data/TNG_set.csv')\n",
    "labels = data_df['SourceID'].copy()\n",
    "locations = data_df['location'].copy()\n",
    "channel_data = data_df.iloc[:, 0:128].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Channel1', 'Channel2', 'Channel3', 'Channel4', 'Channel5', 'Channel6', 'Channel7', 'Channel8', 'Channel9', 'Channel10', 'Channel11', 'Channel12', 'Channel13', 'Channel14', 'Channel15', 'Channel16', 'Channel17', 'Channel18', 'Channel19', 'Channel20', 'Channel21', 'Channel22', 'Channel23', 'Channel24', 'Channel25', 'Channel26', 'Channel27', 'Channel28', 'Channel29', 'Channel30', 'Channel31', 'Channel32', 'Channel33', 'Channel34', 'Channel35', 'Channel36', 'Channel37', 'Channel38', 'Channel39', 'Channel40', 'Channel41', 'Channel42', 'Channel43', 'Channel44', 'Channel45', 'Channel46', 'Channel47', 'Channel48', 'Channel49', 'Channel50', 'Channel51', 'Channel52', 'Channel53', 'Channel54', 'Channel55', 'Channel56', 'Channel57', 'Channel58', 'Channel59', 'Channel60', 'Channel61', 'Channel62', 'Channel63', 'Channel64', 'Channel65', 'Channel66', 'Channel67', 'Channel68', 'Channel69', 'Channel70', 'Channel71', 'Channel72', 'Channel73', 'Channel74', 'Channel75', 'Channel76', 'Channel77', 'Channel78', 'Channel79', 'Channel80', 'Channel81', 'Channel82', 'Channel83', 'Channel84', 'Channel85', 'Channel86', 'Channel87', 'Channel88', 'Channel89', 'Channel90', 'Channel91', 'Channel92', 'Channel93', 'Channel94', 'Channel95', 'Channel96', 'Channel97', 'Channel98', 'Channel99', 'Channel100', 'Channel101', 'Channel102', 'Channel103', 'Channel104', 'Channel105', 'Channel106', 'Channel107', 'Channel108', 'Channel109', 'Channel110', 'Channel111', 'Channel112', 'Channel113', 'Channel114', 'Channel115', 'Channel116', 'Channel117', 'Channel118', 'Channel119', 'Channel120', 'Channel121', 'Channel122', 'Channel123', 'Channel124', 'Channel125', 'Channel126', 'Channel127', 'Channel128']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the set of predictors you want to use to train the model\n",
    "\n",
    "features = list(featdata) # takes the engineered features\n",
    "#features.remove('city')\n",
    "cst = ColumnSelectTransformer(features)\n",
    "\n",
    "print(features)\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: SourceID, dtype: int64"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locations.head()\n",
    "locations.shape\n",
    "labels.head()\n",
    "#channel_data.head()\n",
    "#print(channel_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>1.339682</td>\n",
       "      <td>0.856854</td>\n",
       "      <td>-0.903901</td>\n",
       "      <td>0.496294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>1.910890</td>\n",
       "      <td>0.209279</td>\n",
       "      <td>-1.139383</td>\n",
       "      <td>0.073078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.251106</td>\n",
       "      <td>0.322726</td>\n",
       "      <td>-1.346493</td>\n",
       "      <td>0.608492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-1.779089</td>\n",
       "      <td>1.500157</td>\n",
       "      <td>0.333674</td>\n",
       "      <td>-0.326082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>3.132879</td>\n",
       "      <td>-0.219716</td>\n",
       "      <td>0.911257</td>\n",
       "      <td>-0.479506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>1.577259</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>-0.311167</td>\n",
       "      <td>1.041502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C         D\n",
       "a  1.339682  0.856854 -0.903901  0.496294\n",
       "b  1.910890  0.209279 -1.139383  0.073078\n",
       "c  0.251106  0.322726 -1.346493  0.608492\n",
       "d -1.779089  1.500157  0.333674 -0.326082\n",
       "e  3.132879 -0.219716  0.911257 -0.479506\n",
       "f  1.577259  0.800224 -0.311167  1.041502"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(np.random.randn(6,4),\n",
    "               index=list('abcdef'),                 columns=list('ABCD'))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>-1.779089</td>\n",
       "      <td>1.500157</td>\n",
       "      <td>0.333674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>3.132879</td>\n",
       "      <td>-0.219716</td>\n",
       "      <td>0.911257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>1.577259</td>\n",
       "      <td>0.800224</td>\n",
       "      <td>-0.311167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "d -1.779089  1.500157  0.333674\n",
       "e  3.132879 -0.219716  0.911257\n",
       "f  1.577259  0.800224 -0.311167"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = df1.loc['d':, 'A':'C']\n",
    "f\n",
    "#g = df1.iloc[3:,0:3]\n",
    "#g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelsDFtest = labels.copy()\n",
    "# print(labelsDFtest.shape)\n",
    "# print(labelsDFtest.head())\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 0, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 1, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 2, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 3, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 4, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 5, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# labelsDFtest = labelsDFtest.replace(to_replace= 6, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "# print(labelsDFtest.shape)\n",
    "# print(labelsDFtest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(labelsDFtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "0    Bkg\n",
      "1    Bkg\n",
      "2    Bkg\n",
      "3    Bkg\n",
      "4    Bkg\n",
      "Name: SourceID, dtype: object\n",
      "(39005,)\n",
      "(39005, 1)\n"
     ]
    }
   ],
   "source": [
    "#featkeys = channel_data['SourceIDs']\n",
    "#featkeys = np.reshape(featkeys, (29000,1))\n",
    "\n",
    "#featdata = featdata.drop('SourceIDs', axis=1)\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "\n",
    "channel_data.head()\n",
    "# recast names  remember BA & CO have duel meanings\n",
    "df2 = data_df.copy()\n",
    "df2['SrcID'] = np.nan\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"0\" in name]] = 'Bkg'\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"1\" in name]] = 'HEU'\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"2\" in name]] = 'WGPu'\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"3\" in name]] = 'I131'\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"4\" in name]] = 'Co60'\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"5\" in name]] = 'Tc99m'\n",
    "#df2['SrcID'].iloc[[i for i, name in enumerate(df2.SourceID) if \"6\" in name]] = 'Tc+HEU'\n",
    "\n",
    "labels = labels.replace(to_replace= 0, value='Bkg', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 1, value='HEU', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 2, value='WGPu', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 3, value='I131', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 4, value='Co60', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 5, value='Tc99m', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "labels = labels.replace(to_replace= 6, value='Tc+HEU', inplace=False, limit=None, regex=False, method='pad', axis=None)\n",
    "\n",
    "# for i in range(0,labels.shape[0]):\n",
    "#     if labels[i] == 0:\n",
    "#         labels[i] = 'Bkg'\n",
    "#     elif labels[i] == 1:\n",
    "#         labels[i] = 'HEU'\n",
    "#     elif labels[i] == 2:\n",
    "#         labels[i] = 'WGPu'\n",
    "#     elif labels[i] == 3:\n",
    "#         labels[i] = 'I131'\n",
    "#     elif labels[i] == 4:\n",
    "#         labels[i] = 'Co60'\n",
    "#     elif labels[i] == 5:\n",
    "#         labels[i] = 'Tc99m'\n",
    "#     else:\n",
    "#         labels[i] = 'Tc+HEU'\n",
    "        \n",
    "print(labels.head())\n",
    "print(labels.shape)\n",
    "labels = labels.values.reshape([labels.shape[0],1])\n",
    "#print(labels.head())\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 1)\n",
      "(39005, 128)\n"
     ]
    }
   ],
   "source": [
    "featkeys = labels.copy()\n",
    "featdata = channel_data.copy()\n",
    "\n",
    "print(featkeys.shape)\n",
    "print(featdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will perform cross validations to select hyperparameters for four different models: \n",
    "### full channel, subset channel, full feature, subset feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess dataset, split into training and test part\n",
    "def data_split(data, keys, columns):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, keys, \n",
    "                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(X_train)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(X_test)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transform(dataTRN, dataTEST, columns):\n",
    "    # just transform the already split data    ### This is redundant if you already used the split command!!\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(dataTRN)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(dataTEST)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat = data_split(featdata, featkeys, features)\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat = data_split(featdata, featkeys, subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26133, 128)\n",
      "(12872, 128)\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "(26133, 128)\n",
      "(12872, 128)\n",
      "(39005, 128)\n",
      "(26133, 1)\n",
      "(12872, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_fullfeat.shape)\n",
    "print(X_test_fullfeat.shape)\n",
    "print(y_train_fullfeat.shape)\n",
    "print(y_test_fullfeat.shape)\n",
    "featdataTRN = X_train_fullfeat.copy()\n",
    "featdataTES = X_test_fullfeat.copy()\n",
    "featkeysTRN = y_train_fullfeat.copy()\n",
    "featkeysTES = y_test_fullfeat.copy()\n",
    "print(featdataTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featdata.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12872, 1)\n"
     ]
    }
   ],
   "source": [
    "print(featkeysTES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  only if not splitting ????\n",
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "#y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat = data_transform(featdataTRN, featdataTES, subset_features)\n",
    "#y_train_subfeat, y_test_subfeat = featkeysTRN, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "(12872, 1)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#featkeysTES = 0\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 1)\n",
      "(12872, 1)\n",
      "(26133, 128)\n",
      "(12872, 128)\n"
     ]
    }
   ],
   "source": [
    "# take a quick look, these should have the same number of rows\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "#print(featkeys1.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featdataTES.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the split data with the PCR & PLS in the dataframe\n",
    "\n",
    "#featdata = featdataTRN\n",
    "#featkeys = featkeysTRN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel1</th>\n",
       "      <th>Channel2</th>\n",
       "      <th>Channel3</th>\n",
       "      <th>Channel4</th>\n",
       "      <th>Channel5</th>\n",
       "      <th>Channel6</th>\n",
       "      <th>Channel7</th>\n",
       "      <th>Channel8</th>\n",
       "      <th>Channel9</th>\n",
       "      <th>Channel10</th>\n",
       "      <th>...</th>\n",
       "      <th>Channel119</th>\n",
       "      <th>Channel120</th>\n",
       "      <th>Channel121</th>\n",
       "      <th>Channel122</th>\n",
       "      <th>Channel123</th>\n",
       "      <th>Channel124</th>\n",
       "      <th>Channel125</th>\n",
       "      <th>Channel126</th>\n",
       "      <th>Channel127</th>\n",
       "      <th>Channel128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>134</td>\n",
       "      <td>141</td>\n",
       "      <td>89</td>\n",
       "      <td>71</td>\n",
       "      <td>66</td>\n",
       "      <td>59</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>98</td>\n",
       "      <td>171</td>\n",
       "      <td>139</td>\n",
       "      <td>102</td>\n",
       "      <td>107</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>106</td>\n",
       "      <td>165</td>\n",
       "      <td>134</td>\n",
       "      <td>131</td>\n",
       "      <td>115</td>\n",
       "      <td>84</td>\n",
       "      <td>70</td>\n",
       "      <td>57</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>34</td>\n",
       "      <td>108</td>\n",
       "      <td>179</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>68</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>96</td>\n",
       "      <td>162</td>\n",
       "      <td>170</td>\n",
       "      <td>98</td>\n",
       "      <td>118</td>\n",
       "      <td>93</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Channel1  Channel2  Channel3  Channel4  Channel5  Channel6  Channel7  \\\n",
       "0         5        28        90       150       134       141        89   \n",
       "1         4        30        98       171       139       102       107   \n",
       "2         7        31       106       165       134       131       115   \n",
       "3         3        34       108       179       142       129        85   \n",
       "4        10        31        96       162       170        98       118   \n",
       "\n",
       "   Channel8  Channel9  Channel10     ...      Channel119  Channel120  \\\n",
       "0        71        66         59     ...               0           0   \n",
       "1        99        65         62     ...               0           1   \n",
       "2        84        70         57     ...               0           0   \n",
       "3        85        68         65     ...               0           0   \n",
       "4        93        63         53     ...               0           1   \n",
       "\n",
       "   Channel121  Channel122  Channel123  Channel124  Channel125  Channel126  \\\n",
       "0           1           1           0           0           0           1   \n",
       "1           0           0           0           1           0           0   \n",
       "2           0           0           1           0           0           1   \n",
       "3           0           0           1           0           0           1   \n",
       "4           0           0           1           1           0           0   \n",
       "\n",
       "   Channel127  Channel128  \n",
       "0           3           2  \n",
       "1           1           0  \n",
       "2           2           0  \n",
       "3           3           1  \n",
       "4           0           1  \n",
       "\n",
       "[5 rows x 128 columns]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39005, 1)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featkeys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we will perform cross validations to select hyperparameters for different models: \n",
    "### full 128 channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best value of alpha\n",
    "\n",
    "def alpha_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    alphas = np.logspace(-5, 3, 9)\n",
    "    names = []\n",
    "    for i in alphas:\n",
    "        names.append('alpha ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in alphas:\n",
    "        classifiers.append(MLPRegressor(solver='adam', activation='tanh', alpha=i, random_state=1))\n",
    "\n",
    "    x = alphas\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "    \n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(alphas, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform cross-validation to find best solver\n",
    "\n",
    "def solver_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    solvers = ['lbfgs','sgd','adam']\n",
    "    names = []\n",
    "    for i in solvers:\n",
    "        names.append('solver= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in solvers:\n",
    "        classifiers.append(MLPRegressor(solver=i, activation='tanh', learning_rate='adaptive', alpha=0.1, random_state=1))\n",
    "        \n",
    "    x = [1,2,3]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(solvers, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#perform some cross-validation to find best activation function\n",
    "def activation_CV(columns, X_train, X_test, y_train, y_test):\n",
    "    activators = ['identity', 'logistic', 'tanh', 'relu']\n",
    "    names = []\n",
    "    for i in activators:\n",
    "        names.append('activator= ' + str(i))\n",
    "\n",
    "    classifiers = []\n",
    "    for i in activators:\n",
    "        classifiers.append(MLPRegressor(solver='adam', activation=i, alpha=0.1, random_state=1))\n",
    "\n",
    "    x = [1,2,3,4]\n",
    "    y = []\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        y.append(score)\n",
    "        #print(clf, score)\n",
    "\n",
    "    #plt.plot(x, y)\n",
    "    #plt.show()\n",
    "    return(zip(activators, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess dataset, split into training and test part\n",
    "def data_split(data, keys, columns):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, keys, \n",
    "                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(X_train)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(X_test)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std, y_train, y_test)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_transform(dataTRN, dataTEST, columns):\n",
    "    # just transform the already split data\n",
    "\n",
    "    cst = ColumnSelectTransformer(columns)\n",
    "    X_train1 = cst.transform(dataTRN)\n",
    "    X_train_std = StandardScaler().fit_transform(X_train1)\n",
    "    X_test1 = cst.transform(dataTEST)\n",
    "    X_test_std = StandardScaler().fit_transform(X_test1)\n",
    "\n",
    "    return(columns, X_train_std, X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat = data_split(featdata, featkeys, features)\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat = data_split(featdata, featkeys, subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "(12872, 1)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "#featkeysTES = 0\n",
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "print(featkeysTES.shape)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#col_fullfeat, X_train_fullfeat, X_test_fullfeat = data_transform(featdataTRN, featdataTES, features)\n",
    "#y_train_fullfeat, y_test_fullfeat = featkeys, featkeysTES\n",
    "#col_subfeat, X_train_subfeat, X_test_subfeat = data_transform(featdataTRN, featdataTES, subset_features)\n",
    "#y_train_subfeat, y_test_subfeat = featkeysTRN, featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Bkg'],\n",
       "       ['Tc99m'],\n",
       "       ['Bkg'],\n",
       "       ..., \n",
       "       ['Bkg'],\n",
       "       ['Tc99m'],\n",
       "       ['HEU']], dtype=object)"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featkeysTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'I131'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-263-ad3ede166f2d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malpha_full_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malpha_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_fullfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha_full_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msolver_full_feat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msolver_CV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_fullfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_fullfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver_full_feat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-253-e85e91523e1c>\u001b[0m in \u001b[0;36malpha_CV\u001b[1;34m(columns, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m# iterate over classifiers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifiers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtrained\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m--> 620\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    329\u001b[0m                              hidden_layer_sizes)\n\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py\u001b[0m in \u001b[0;36m_validate_input\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m   1302\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincremental\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[1;32m-> 1304\u001b[1;33m                          multi_output=True, y_numeric=True)\n\u001b[0m\u001b[0;32m   1305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'O'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'I131'"
     ]
    }
   ],
   "source": [
    "alpha_full_feat = alpha_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat)\n",
    "print(list(alpha_full_feat))\n",
    "\n",
    "solver_full_feat = solver_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat)\n",
    "print(list(solver_full_feat))\n",
    "\n",
    "activation_full_feat = activation_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat)\n",
    "print(list(activation_full_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_sub_feat = alpha_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat)\n",
    "print(list(alpha_sub_feat))\n",
    "\n",
    "solver_sub_feat = solver_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat)\n",
    "print(list(solver_sub_feat))\n",
    "\n",
    "activation_sub_feat = activation_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat)\n",
    "print(list(activation_sub_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finallly we will train four different models on the full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just read in data from files and over-write\n",
    "# featkeysTRN = pd.read_csv('/home/jack/projects/dengue/data/sj_train100per_labels.csv')\n",
    "\n",
    "# featdataTRN = pd.read_csv('/home/jack/projects/dengue/data/sj_train100per.csv')\n",
    "# featdataTES = pd.read_csv('/home/jack/projects/dengue/data/sj_testTrue.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39005, 128)\n",
      "(39005, 1)\n",
      "(26133, 128)\n",
      "(26133, 1)\n",
      "(12872, 128)\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(featdata.shape)\n",
    "print(featkeys.shape)\n",
    "print(featdataTRN.shape)\n",
    "print(featkeysTRN.shape)\n",
    "print(featdataTES.shape)\n",
    "\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Pipeline which you can use to train and predict\n",
    "# Step 1: take the data (training or testing) and select only the columns of interest\n",
    "# Step 2: transform all of the features to Standard Variables\n",
    "# Step 3: feed the data into a Multi-Layer Perceptron \n",
    "\n",
    "NN_full_feat = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(features)),\n",
    "    ('sdt', StandardScaler()),\n",
    "    ('mlp', MLPRegressor(solver='adam', activation='relu', alpha=100.0)) #, alpha=0.1))\n",
    "    ])\n",
    "\n",
    "RF_full_feat = Pipeline([\n",
    "    ('cst', ColumnSelectTransformer(features)),\n",
    "    ('sdt', StandardScaler()),\n",
    "    ('rfr', RandomForestRegressor(n_estimators=800))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'filter'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-270-56c28575a3fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTRN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeatkeysTRN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0my_pred5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRF_full_feat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatdataTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-232-36dfdbfc867f>\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m# return a new array with just the columns you specify\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mnewarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnewarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'filter'"
     ]
    }
   ],
   "source": [
    "params =  {'rfr__n_estimators':800, 'rfr__max_features': 65}\n",
    "RF_full_feat.set_params(**params)\n",
    "\n",
    "RF_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred5 = RF_full_feat.predict(featdataTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred4 = NN_subset_feat.predict(featdataTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(y_pred5))\n",
    "len(y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing\n",
    "X_train_feat, X_test_feat, y_train_feat, y_test_feat = train_test_split(featdata, featkeys, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'featkeysTES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-2fde95deabdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatkeysTES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'featkeysTES' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(featkeysTES))\n",
    "len(subset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NN_full_feat.set_params(alpha = 100)\n",
    "NN_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred3 = NN_full_feat.predict(featdataTES)\n",
    "\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "# Save the trained model\n",
    "#joblib.dump(NN_full_feat, '/Users/turk_la/Documents/SSAM/Data/UF6data/NN_full_features.pkl')\n",
    "\n",
    "y_test_feat['pred'] = y_pred3\n",
    "y_test_feat['dif'] = abs(y_test_feat['x'] - y_test_feat['pred'])\n",
    "y_test_feat['dif'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred4 = NN_subset_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred4'] = y_pred4\n",
    "y_test_feat['dif_sub'] = abs(y_test_feat['x'] - y_test_feat['pred4'])\n",
    "y_test_feat['dif_sub'].sum()/len(featkeysTES)\n",
    "\n",
    "# Save the trained model\n",
    "#joblib.dump(NN_subset_feat, '/Users/turk_la/Documents/SSAM/Data/UF6data/NN_sub_features.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try the Random Forest to do the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params =  {'rfr__n_estimators':800, 'rfr__max_features': 65}\n",
    "RF_subset_feat.set_params(**params)\n",
    "\n",
    "RF_subset_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred5 = RF_subset_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred5'] = y_pred5\n",
    "y_test_feat['dif_sub_rf'] = abs(y_test_feat['x'] - y_test_feat['pred5'])\n",
    "y_test_feat['dif_sub_rf'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_full_feat.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#params = ['n_estimators': 800, 'max_features': 165\n",
    "params =  {'rfr__n_estimators':1200, 'rfr__max_features': 365}\n",
    "RF_full_feat.set_params(**params)\n",
    "RF_full_feat.fit(featdataTRN,featkeysTRN)\n",
    "y_pred6 = RF_full_feat.predict(featdataTES)\n",
    "\n",
    "y_test_feat = featkeysTES\n",
    "\n",
    "y_test_feat['pred6'] = y_pred6\n",
    "y_test_feat['dif_sub_rf'] = abs(y_test_feat['x'] - y_test_feat['pred6'])\n",
    "y_test_feat['dif_sub_rf'].sum()/len(featkeysTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred3\n",
    "print(y_pred4.shape)\n",
    "w = sum(y_pred4)\n",
    "print(w/260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## making  submission\n",
    "#submission = pd.read_csv('/home/jack/projects/dengue/data/submission_format.csv')\n",
    "\n",
    "# submission 13 for Iq\n",
    "submission = pd.read_csv('/home/jack/projects/dengue/results/predictions13_EM.csv')\n",
    "\n",
    "count = 0\n",
    "for i in y_pred4:\n",
    "    if i < 0.0:\n",
    "        print(i)\n",
    "        submission.loc[count,'total_cases'] = 0.0\n",
    "    else:\n",
    "        submission.loc[count,'total_cases'] = i\n",
    "    count += 1\n",
    "\n",
    "submission.to_csv('/home/jack/projects/dengue/results/predictions29_EM.csv',index='FALSE')\n",
    "    \n",
    "#submission.total_cases = np.concatenate([y_pred6])\n",
    "    \n",
    "#submission.head()\n",
    "#submission.tail()\n",
    "#submission.shape\n",
    "#submission.loc[411, 'weekofyear']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred4[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(col_subfeat), X_train_subfeat.shape, X_test_subfeat.shape, y_train_subfeat.shape, y_test_subfeat.shape)\n",
    "y_test_subfeat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest_sub_feat = forest_CV(col_subfeat, X_train_subfeat, X_test_subfeat, y_train_subfeat, y_test_subfeat['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "forest_fullfeat = forest_CV(col_fullfeat, X_train_fullfeat, X_test_fullfeat, y_train_fullfeat, y_test_fullfeat['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Experimenting with implementing a time series aspect\n",
    "\n",
    "X_tr, X_ts, y_tr, y_ts = featdata[:800], featdata[800:], featkeys[:800], featkeys[800:]\n",
    "\n",
    "RF_full_feat.fit(X_tr,y_tr)\n",
    "\n",
    "preds = []\n",
    "\n",
    "new_val= y_tr['x'][799]\n",
    "\n",
    "for i in range(936-800):\n",
    "\n",
    "    #print(new_val)\n",
    "\n",
    "    X_ts.loc[i+800,'prior_val'] = float(new_val)\n",
    "\n",
    "    #print(X_ts.loc[i+800,'prior_val'])\n",
    "\n",
    "    datum = X_ts[i:i+1]\n",
    "\n",
    "    #print(datum)    \n",
    "\n",
    "    new_val = RF_full_feat.predict(datum)\n",
    "\n",
    "    preds.append(float(new_val))\n",
    "\n",
    "    \n",
    "\n",
    "y_ts['pred'] = preds\n",
    "\n",
    "y_ts['dif'] = abs(y_ts['x'] - y_ts['pred'])\n",
    "\n",
    "y_ts['dif'].sum()/len(y_ts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
